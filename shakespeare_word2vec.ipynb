{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b76b32d-e202-42dd-8088-e2ff22b08aa0",
   "metadata": {},
   "source": [
    "# Wrod2Vec Implementation:\n",
    "###### Yassin Bahid\n",
    "\n",
    "\n",
    "It is said that the complete work of Shakespear contains evry single human emotion. What better source then for a word2vec dictionary. We focus on the Sonnets only and implement a word2vec algorithm with a regular sigmoid Function. We will then look at the similariy, or closeness, of certain words after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "293f4b17-e6b7-4870-aaec-27683cfc1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b2b1a-8738-432c-9059-689ad127f9f4",
   "metadata": {},
   "source": [
    "## Data:\n",
    "\n",
    "We use the MIT data base of Shakespear work: http://shakespeare.mit.edu/. We clean the data by eliminating \"'s\", puncruations and line breakes. When considering words that are closest to each other and related, we only consider each verse as a sentence, i.e., we do not consider the word at the end of a verse, and the word at the begining of the following verse as \"close\". We also add negative matches in the form of random choices from the entire body of text. The data is not stemmed as of Yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987e277d-aa20-4ad7-a4ac-ab6790d4d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting stopwords\n",
    "with open('stopwords.txt') as f:\n",
    "    stopwords = f.read().replace('\\n',' ').split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e307bc2-c472-4354-9153-461bdd8d786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TheSonnets.txt') as Shake_Sonnets:\n",
    "    lines = Shake_Sonnets.readlines()\n",
    "##cleaning the data:\n",
    "for line_index in range(0, len(lines)):\n",
    "    \n",
    "    lines[line_index] = lines[line_index].replace(\"\\n\", \"\")\n",
    "    lines[line_index] = lines[line_index].replace(\"-\", \" \")\n",
    "    lines[line_index] = lines[line_index].replace(\"'s\", \"\")\n",
    "    lines[line_index] = lines[line_index].replace(\"  \", \" \")\n",
    "    lines[line_index] = lines[line_index].replace(\",\", \"\")\n",
    "    lines[line_index] = lines[line_index].replace(\"'\", \"\")\n",
    "    \n",
    "    if lines[line_index][0:2]==\"  \":\n",
    "        lines[line_index] = lines[line_index][2:]\n",
    "    \n",
    "    for i in range(0,9):\n",
    "        if str(i) in lines[line_index]:\n",
    "            lines[line_index] = \"\"\n",
    "    lines[line_index] = lines[line_index].lower()\n",
    "    \n",
    "    lines[line_index] = lines[line_index].split()\n",
    "    \n",
    "    lines[line_index] = [ wor for wor in lines[line_index] if wor not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468aaf26-f06d-4cfd-9352-e0e318d41f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set([w for i in range(0,len(lines))  for w in lines[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14c474cb-525e-4e61-8864-7c85bdb4acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the training data:\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "NUM_NEGATIVE_SAMPLES = 2\n",
    "\n",
    "data = []\n",
    "\n",
    "for ln in lines:\n",
    "    for idx, center_word in enumerate(ln[WINDOW_SIZE-1:-WINDOW_SIZE+1]):\n",
    "        ## Getting the context words:\n",
    "        context_words = [context_word \n",
    "                             for context_word \n",
    "                             in ln[idx:idx+2*WINDOW_SIZE-1] \n",
    "                             if context_word != center_word]\n",
    "        \n",
    "        for context_word in context_words:\n",
    "            data = data + [[center_word, context_word, 1]]\n",
    "        ## Getting negative samples for words not present around the center word:\n",
    "        if [w for w in all_words[WINDOW_SIZE-1:-WINDOW_SIZE] if w != center_word and w not in context_words] != []:\n",
    "            negative_samples = np.random.choice(\n",
    "                                        [w for w \n",
    "                                         in all_words[WINDOW_SIZE-1:-WINDOW_SIZE] \n",
    "                                         if w != center_word\n",
    "                                         and w not in context_words],\n",
    "                                        NUM_NEGATIVE_SAMPLES)\n",
    "            for negative_samp in negative_samples:\n",
    "\n",
    "                data.append([center_word, negative_samp, 0])\n",
    "\n",
    "sonnet_df = pd.DataFrame(columns=['center_word', 'context_word', 'label'], data=data)\n",
    "words = np.intersect1d(sonnet_df.context_word, sonnet_df.center_word)\n",
    "sonnet_df = sonnet_df[(sonnet_df.center_word.isin(words)) & (sonnet_df.context_word.isin(words))].reset_index(drop=True)\n",
    "                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "912a2d08-9e36-41a7-9fe8-085b3090d215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center_word</th>\n",
       "      <th>context_word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>creatures</td>\n",
       "      <td>fairest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>creatures</td>\n",
       "      <td>desire</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>creatures</td>\n",
       "      <td>heavenly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>desire</td>\n",
       "      <td>creatures</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>desire</td>\n",
       "      <td>destroys</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15923</th>\n",
       "      <td>water</td>\n",
       "      <td>rigour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15924</th>\n",
       "      <td>water</td>\n",
       "      <td>cools</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15925</th>\n",
       "      <td>water</td>\n",
       "      <td>dully</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15926</th>\n",
       "      <td>water</td>\n",
       "      <td>pursuit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15927</th>\n",
       "      <td>cools</td>\n",
       "      <td>water</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15928 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      center_word context_word  label\n",
       "0       creatures      fairest      1\n",
       "1       creatures       desire      1\n",
       "2       creatures     heavenly      0\n",
       "3          desire    creatures      1\n",
       "4          desire     destroys      0\n",
       "...           ...          ...    ...\n",
       "15923       water       rigour      0\n",
       "15924       water        cools      1\n",
       "15925       water        dully      0\n",
       "15926       water      pursuit      0\n",
       "15927       cools        water      1\n",
       "\n",
       "[15928 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnet_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdd3d2-5eca-4d0b-8a00-345ff7a1da37",
   "metadata": {},
   "source": [
    "## Word2Vec Code:\n",
    "\n",
    "We use the data generated in the previous section to create an embedding, or vector, representing each word. Words that appear close to each other whould have vectors that are the closest. All these embeddings shall be inside the unit circle. Now each words will have a main embeding and a context embedding. The first is the actual vector space representation of each word. The second, is the vector representation of the word as it appears in the context. We use the context vectors to update the main embeddings. The score is computed from the sigmoid of the dot product of the context and main vector. If the two vectors are close the sigmoid will be close to 1, Butis they are far from each other , it will be 0. The error is then the difference betwwen the actual label and the error. We update the main vector by moving the the vectors either closer to each other, or further from each other deppending. The benefit of this method is two folds: First, it moves the main embeddings and context embeddings towards each other. Second, it moves main vectors that have similar context words closer to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87d9263d-a631-4f11-a8ff-0b31f71897ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trainig the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eccde488-c877-4746-ab58-39d008e51e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v, scale=1):\n",
    "    return 1 / (1 + np.exp(-scale*v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5dab18c-0ef3-42e8-8b48-0ddde223ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(df, main_embeddings, context_embeddings, learning_rate, debug=False):\n",
    "    \n",
    "    #get differences between main embeddings and corresponding context embeddings\n",
    "    main_embeddings = main_embeddings.loc[df.center_word].values\n",
    "    context_embeddings = context_embeddings.loc[df.context_word].values\n",
    "    diffs = context_embeddings_context - main_embeddings_center\n",
    "    \n",
    "    #get similarities, scores, and errors between main embeddings and corresponding context embeddings\n",
    "    dot_prods = np.sum(main_embeddings * context_embeddings, axis=1)\n",
    "    scores = sigmoid(dot_prods)\n",
    "    errors = (df.label - scores).values.reshape(-1,1)\n",
    "    \n",
    "    #calculate updates\n",
    "    updates = diffs*errors*learning_rate\n",
    "    updates_df = pd.DataFrame(data=updates)\n",
    "    updates_df['center_word'] = df.center_word\n",
    "    updates_df['context_word'] = df.context_word\n",
    "    updates_df_center = updates_df.groupby('center_word').sum()\n",
    "    updates_df_context = updates_df.groupby('context_word').sum()\n",
    "    \n",
    "    #apply updates\n",
    "    main_embeddings += updates_df_center.loc[main_embeddings.index]\n",
    "    context_embeddings -= updates_df_context.loc[context_embeddings.index]\n",
    "    \n",
    "    #normalize embeddings\n",
    "    main_embeddings = normalize_data(main_embeddings)\n",
    "    context_embeddings = normalize_data(context_embeddings)\n",
    "    \n",
    "    #return the updated embeddings\n",
    "    return main_embeddings, context_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0723db-82ce-4964-9e6c-271b08d9df7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9396f-d527-4e7a-9224-18a5d975f724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "583cd185-17d1-4d03-a745-39ce18cbdb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "main_embeddings = np.random.normal(0,0.1,(len(words), EMBEDDING_SIZE))\n",
    "row_norms = np.sqrt((main_embeddings**2).sum(axis=1)).reshape(-1,1)\n",
    "main_embeddings = main_embeddings / row_norms\n",
    "\n",
    "context_embeddings = np.random.normal(0,0.1,(len(words), EMBEDDING_SIZE))\n",
    "row_norms = np.sqrt((context_embeddings**2).sum(axis=1)).reshape(-1,1)\n",
    "context_embeddings = context_embeddings / row_norms\n",
    "\n",
    "main_embeddings = pd.DataFrame(data=main_embeddings, index=words)\n",
    "context_embeddings = pd.DataFrame(data=context_embeddings, index=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5fa7d6-f581-48ec-9fe2-cd2027c95190",
   "metadata": {},
   "source": [
    "### Results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a772cb4e-85f9-485b-bd3d-846f3e1ad622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 'loves', 0.47626850299071566),\n",
       " ('love', 'saturn', 0.4304067248743966),\n",
       " ('love', 'quill', 0.42985441939037183),\n",
       " ('love', 'mistress', 0.4246136590489624),\n",
       " ('love', 'temptation', 0.422496543854154),\n",
       " ('love', 'willed', 0.4064902775730783),\n",
       " ('love', 'less', 0.39182032918226883),\n",
       " ('love', 'wink', 0.36580739655958316),\n",
       " ('love', 'feathered', 0.3559349176205441),\n",
       " ('love', 'soundless', 0.35249907387580737)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = []\n",
    "for w1 in words:\n",
    "    for w2 in words:\n",
    "        if w1 != w2:\n",
    "            sim = 1 - cosine(main_embeddings.loc[w1], main_embeddings.loc[w2])\n",
    "            L.append((w1,w2,sim))\n",
    "sorted([item for item in L if item[0] == 'love'], key=lambda t: -t[2])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25eda480-b2f8-439c-a13b-5a31c54def28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 'replete', -0.3608068036960259),\n",
       " ('love', 'argument', -0.36246532523482977),\n",
       " ('love', 'shalt', -0.36349737452243347),\n",
       " ('love', 'believed', -0.36554522130009204),\n",
       " ('love', 'travail', -0.36895822444962434),\n",
       " ('love', 'weeds', -0.3727390968083115),\n",
       " ('love', 'carve', -0.37726877337258946),\n",
       " ('love', 'express', -0.38897830275574696),\n",
       " ('love', 'concealed', -0.396334234335856),\n",
       " ('love', 'pencil)', -0.4374159622020506)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([item for item in L if item[0] == 'love'], key=lambda t: -t[2])[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc417e-183f-47ba-ac8d-8b66c80686ed",
   "metadata": {},
   "source": [
    "\n",
    "We focus here on one word: Love. We see what the 10 closest words to it are and the 10 furthest. Interestingly, loves is the closest word. While these two words are unlikely to appear next to each other, they do have very similar context words. One possible  way to remedy this problem is stemming to reduce every word to its root. We can also see that love for Shakespeare is mostly related to temptation, concealed, mistress, and Saturn. Saturn is often paired with Venus symbolizing love in shakespearean literature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a7349-c060-4d13-8044-4bdde97f95b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
